---
title: "p8105_hw3_smm2350"
author: "Shaina Mackin" 
date: "2021-10-20"
output: github_document
---

# Homework 3

To begin, I will load packages and instacart data, and set theme options.

```{r libraries, echo = FALSE, message = FALSE}
library(tidyverse)
library(p8105.datasets)
library(lubridate)
library(janitor)
library(readxl)
data("instacart")

theme_set(theme_minimal() + theme(legend.position = "bottom"))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1

### Instacart data

I will write a short description of the dataset,
noting the size and structure of the data, 
describing key variables, 
and giving illustrative examples of observations:

The instacart dataset contains online grocery shopping data from 2017, with 
`r nrow(instacart)` observations, or products from orders, and `r ncol(instacart)` 
variables, for a total of `r (nrow(instacart)*ncol(instacart))` data points. 
Key variables include `order_id`, `product_id`, `user_id`, whether items were 
`reordered`, `order_hour_of_day` and `days_since_prior_order` (ranging from 
`r min(pull(instacart, days_since_prior_order))` to `r max(pull(instacart, 
days_since_prior_order))` ), as well as `product_name` and associated `aisle` 
and `department`.

Initial analysis shows `produce` to be the most frequently ordered-from department, 
as visualized below: 

```{r exploration}
instacart %>%
  count(department) %>%
  arrange(desc(n)) %>%
  ggplot(aes(forcats::fct_reorder(department, (n)), n)) +
  geom_col() +
  coord_flip() +
  labs(
    title = "Departments in decreasing order of popularity",
    x = "Department name",
    y = "Number of items ordered",
  )
```

#### Let's explore how many aisles are there, and which are most ordered from.

```{r aisles_count}
instacart %>%
  group_by(aisle_id, aisle) %>%
  summarize(n_obs = n()) %>%
  arrange(desc(n_obs)) 
```

There are 134 aisles. The aisles with the most ordered-from items are 
`fresh vegetables` and `fresh fruits`, followed by `packaged vegetables fruits`. 

#### To illustrate this, I will make a plot that shows the number of items 
#### ordered in each aisle, limiting to aisles with more than 10000 items ordered.

```{r aisles_plot}
instacart %>%
  count(aisle) %>%
  filter(n > 10000) %>%
  ggplot(aes(forcats::fct_reorder(aisle, (n)), n)) +
  geom_col() +
  coord_flip() +
  labs(
    title = "Number of items ordered per aisle",
    x = "Aisle name",
    y = "Number of items",
    caption = "note: limited to aisles with more than 10,000 items ordered"
  )
```

This again shows that the aisles most ordered from are `fresh vegetables` 
and `fresh fruits`, followed by `packaged vegetables fruits`.

#### I will make a table showing the three most popular items in the 
`baking ingredients`, `dog food care`, and `packaged vegetables fruits` aisles. 

```{r aisles_table}
instacart %>%
  filter(aisle == "baking ingredients" | 
           aisle == "dog food care" | 
           aisle == "packaged vegetables fruits") %>%
 count(aisle, product_name) %>%
  arrange(desc(n)) %>%
  group_by(aisle) %>%
  slice(1:3) %>%
  arrange(desc(n)) %>%
  group_by(aisle) %>%
  knitr::kable(caption = "Top  3  Items Per Aisle")
```

We see that `organic baby spinach`, `organic raspberries`, and `organic blueberries` 
are the top three ordered items from the `packaged vegetables fruits` aisle; 
`light brown sugar`, `pure baking soda`, and `cane sugar` are the top three ordered 
items from the `baking ingredients` aisle; 
and `snack sticks chicken & rice recipe dog treats`, 
`organix chicken & brown rice recipe`, and `small dog biscuits` are the top three
ordered items from the `dog food care` aisle, respectively.

#### I will make a 2x7 table showing the mean hour of the day at which 
#### Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week. 

```{r apples_ice_table}
instacart %>%
  mutate(t_day_number = order_dow +1) %>%
  mutate(day_of_week = wday(t_day_number, label = TRUE)) %>%
  filter(product_name == "Pink Lady Apples" | product_name == "Coffee Ice Cream") %>%
  group_by(product_name, day_of_week) %>%
  summarize(mean_hour = round(mean(order_hour_of_day), digits = 2)) %>%
  pivot_wider(
    names_from = day_of_week,
    values_from = mean_hour
  ) %>%
  knitr::kable(caption = "Mean Hour of Day Ordered")
```

`Coffee Ice Cream` is ordered, on average, later in the day than `Pink Lady Apples`. 

## Problem 2

### BRFSS data

I will clean the data, formatting to use appropriate variable names, 
focusing on the `Overall Health` topic, and only including factored responses 
ordered from `Excellent` to `Poor`.

```{r brfss_tidy}
data("brfss_smart2010")

brfss_tidy = brfss_smart2010 %>%
  janitor::clean_names() %>%
   filter(
    topic %in% "Overall Health",
    response %in% "Poor" | response %in% "Fair" | response %in% "Good" | 
      response %in% "Very good" | response %in% "Excellent") %>% 
 mutate(response_ordered = factor(response, ordered = TRUE, 
        levels = c("Poor", "Fair", "Good", "Very good", "Excellent")))
```

#### Let's determine which states were observed at 7+ locations in 2002 and 2010.

```{r brfss_locations}
brfss_tidy %>%
  select(year, locationabbr, locationdesc) %>%
  filter(year == 2002) %>%
  distinct() %>%
  group_by(locationabbr) %>%
  filter(n() >= 7) %>%
  select(locationabbr) %>%
  distinct()

brfss_tidy %>%
  select(year, locationabbr, locationdesc) %>%
  filter(year == 2010) %>%
  distinct() %>%
  group_by(locationabbr) %>%
  filter(n() >= 7) %>% 
  select(locationabbr) %>%
  distinct() 
```

In 2002, `CT`, `FL`, `MA`, `NJ`, `NC`, and `PA` were observed at 7 or more locations. 
In 2010, `CA`, `CO`, `FL`, `MD`, `MA`, `NE`, `NJ`, `NY`, `NC`, `OH`, `PA`, 
`SC`, `TX`, and `WA` were observed at 7 or more locations.  

#### I will construct a dataset that is limited to Excellent responses, 
#### and contains, year, state, and a variable that averages the data_value across 
#### locations within a state. I will also make a “spaghetti” plot of this 
#### average value over time within a state.

```{r brfss_excellent}
df_excellent = brfss_tidy %>%
  filter(response %in% "Excellent") %>%
  select(year, locationabbr, data_value) %>%
  unique() %>%
  na.omit() %>%
  group_by(locationabbr, year) %>%
  mutate(
    avg_data_value = mean(data_value)) %>%
  select(year, locationabbr, avg_data_value) %>%
  distinct()

df_excellent %>%
  ggplot(aes(x = year, y = avg_data_value)) +
  geom_line(aes(group = locationabbr, color = locationabbr)) +
  labs(
    title = "Average data value among states over time",
    x = "Year",
    y = "Average Data Value") +
  guides(col=guide_legend("State")) +
  theme(legend.position = "right")
```

The state with the lowest average data value over the years is `WY`.

#### I will make a two-panel plot showing, for the years 2006, and 2010, 
#### distribution of data_value for responses (“Poor” to “Excellent”) 
#### among locations in NY State.

```{r brfss_excellent_plot}
brfss_tidy %>%
  select(year, data_value, locationabbr, locationdesc, response_ordered) %>%
  filter(
    year == 2006 | year == 2010,
    locationabbr == "NY") %>%
  ggplot(aes(x = data_value, fill = response_ordered)) +
  geom_density(aes(fill = response_ordered), alpha = .5) + 
  labs(
    title = "Data value across responses among NY locations",
    x = "Data Value",
    fill = "Response"
  ) +
  theme(
    axis.title.y = element_blank(),
    axis.text.y = element_blank()) +
  facet_grid(. ~ year) +
   guides(col=guide_legend("Location"))
```

For both `2006` and `2010`, the highest `data_value` among all NY locations 
was associated with a `Good` `response`. 

## Problem 3

### Accelerometer data

First, I will load and tidy the accelerometer data.

```{r accel_import}
accel_df = read_csv("accel_data.csv") %>%
  janitor::clean_names() %>%
  mutate(
    day = factor(day, ordered = TRUE,
                 levels = c("Monday", "Tuesday", "Wednesday", "Thursday", 
                            "Friday", "Saturday", "Sunday")),
    week = factor(week, ordered = TRUE,
                  levels = c(1:5)),
    weekend_or_weekday = ifelse(day %in% c("Saturday", "Sunday"), 
                                "weekend", "weekday") %>%
    factor(levels = c("weekday", "weekend"))) %>%
    pivot_longer(
    activity_1:activity_1440,
    names_to = "minute",
    names_prefix = "activity_",
    values_to = "count")
```

The tidied `accel_df` contains `r nrow(accel_df)` rows (or observations), 
one for every `minute` of every `day` from the `r max(pull(accel_df, week))` `week` 
period of accelerometer data collection. 
The dataset's `r ncol(accel_df)` variables include the following: 
`r colnames(accel_df)`.

#### Daily activity

I will aggregate across minutes to create a total activity variable for each day, 
and create a table showing these totals. 

```{r accel_daily_activity}
daily_df = accel_df %>%
  group_by(day_id) %>%
  summarize(activity = sum(count)) %>%
  mutate(day_id = paste("day", day_id)) %>%
  pivot_wider(
    names_from = day_id,
    values_from = activity)

knitr::kable(daily_df, caption = "Total Daily Activity")
```

TRENDS? 

#### 24-hour plot

I will make a single-panel plot that shows the 24-hour activity time courses for 
each day and use color to indicate day of the week. 

```{r accel_24_plot, message=FALSE}
panel_df = accel_df %>%
  mutate(week = paste("week", week)) %>%
  ggplot(aes(x = minute, y = count, color = day)) +
  geom_point(size = .5) +
  geom_line() +
  labs(
    title = "24-hour accelerometer activity time courses by day",
    x = "minute",
    y = "activity count"
  ) +
   theme(legend.position = "right")
```

DESCRIBE!
